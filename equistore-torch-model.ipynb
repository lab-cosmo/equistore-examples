{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b5c7c0",
   "metadata": {
    "hide_input": true,
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Manipulate-Hamiltonian-into-blocks\" data-toc-modified-id=\"Manipulate-Hamiltonian-into-blocks-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Manipulate Hamiltonian into blocks</a></span></li><li><span><a href=\"#Feature-computation\" data-toc-modified-id=\"Feature-computation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Feature computation</a></span></li><li><span><a href=\"#Feature-Preprocessing\" data-toc-modified-id=\"Feature-Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Feature Preprocessing</a></span></li><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#DataLoader\" data-toc-modified-id=\"DataLoader-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>DataLoader</a></span></li><li><span><a href=\"#Model\" data-toc-modified-id=\"Model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model</a></span></li><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Evaluation</a></span></li><li><span><a href=\"#Tests\" data-toc-modified-id=\"Tests-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Tests</a></span><ul class=\"toc-item\"><li><span><a href=\"#set-up-wigner-d-rotations-matrices\" data-toc-modified-id=\"set-up-wigner-d-rotations-matrices-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>set up wigner-d rotations matrices</a></span></li><li><span><a href=\"#block-wise-rotations\" data-toc-modified-id=\"block-wise-rotations-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>block wise rotations</a></span></li><li><span><a href=\"#Rotate-matrix\" data-toc-modified-id=\"Rotate-matrix-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Rotate matrix</a></span></li><li><span><a href=\"#Eigenvalue-tests\" data-toc-modified-id=\"Eigenvalue-tests-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;</span>Eigenvalue tests</a></span></li><li><span><a href=\"#check-decoupling-of-blocks\" data-toc-modified-id=\"check-decoupling-of-blocks-9.5\"><span class=\"toc-item-num\">9.5&nbsp;&nbsp;</span>check decoupling of blocks</a></span></li><li><span><a href=\"#check-feature-rotations\" data-toc-modified-id=\"check-feature-rotations-9.6\"><span class=\"toc-item-num\">9.6&nbsp;&nbsp;</span>check feature rotations</a></span></li><li><span><a href=\"#mean-Wei-Bin's-tests\" data-toc-modified-id=\"mean-Wei-Bin's-tests-9.7\"><span class=\"toc-item-num\">9.7&nbsp;&nbsp;</span>mean Wei Bin's tests</a></span><ul class=\"toc-item\"><li><span><a href=\"#meanless-hamiltonian\" data-toc-modified-id=\"meanless-hamiltonian-9.7.1\"><span class=\"toc-item-num\">9.7.1&nbsp;&nbsp;</span>meanless hamiltonian</a></span></li><li><span><a href=\"#meanless-blocks\" data-toc-modified-id=\"meanless-blocks-9.7.2\"><span class=\"toc-item-num\">9.7.2&nbsp;&nbsp;</span>meanless blocks</a></span></li><li><span><a href=\"#stupid-test\" data-toc-modified-id=\"stupid-test-9.7.3\"><span class=\"toc-item-num\">9.7.3&nbsp;&nbsp;</span>stupid test</a></span></li></ul></li></ul></li><li><span><a href=\"#Jigyasa-needs-to-modify-samples-for-features-to-start-from-1\" data-toc-modified-id=\"Jigyasa-needs-to-modify-samples-for-features-to-start-from-1-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Jigyasa needs to modify samples for features to start from 1</a></span></li><li><span><a href=\"#Old-and-existing\" data-toc-modified-id=\"Old-and-existing-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Old and existing</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3096d21b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T16:48:51.875569Z",
     "start_time": "2022-12-27T16:48:51.871591Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0b2bb576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T09:50:27.096818Z",
     "start_time": "2022-12-29T09:50:27.081178Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import ase.io\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "# from rascal.representations import SphericalExpansion\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from ase.units import Hartree\n",
    "\n",
    "from torch_hamiltonian_utils.torch_cg import ClebschGordanReal\n",
    "from torch_hamiltonian_utils.torch_hamiltonians import fix_pyscf_l1, lowdin_orthogonalize, dense_to_blocks, blocks_to_dense, couple_blocks, decouple_blocks, hamiltonian_features\n",
    "from torch_hamiltonian_utils.torch_builder import TensorBuilder\n",
    "\n",
    "import equistore\n",
    "from equistore import Labels, TensorBlock, TensorMap\n",
    "from equistore_utils.librascal import  RascalSphericalExpansion, RascalPairExpansion\n",
    "from equistore_utils.acdc_mini import acdc_standardize_keys, cg_increment, cg_combine\n",
    "from equistore_utils.model_hamiltonian import get_feat_keys, get_feat_keys_from_uncoupled \n",
    "\n",
    "import importlib\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "d68aeb81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:32:57.526225Z",
     "start_time": "2022-12-29T10:32:57.519926Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_feat_keys_from_uncoupled(block_keys, sigma=None, order_nu=None):\n",
    "    \"\"\"Map UNCOUPLED block keys to corresponding feature key. take as extra input the sigma, nu value if required.\n",
    "    sigma=0 returns all possible sigma values at given 'nu'\"\"\"\n",
    "    blocktype, species1, n1, l1, species2, n2, l2 = block_keys\n",
    "    feat_blocktype = blocktype\n",
    "    keys_L=[]\n",
    "    for L in range(abs(l1-l2), l1+l2+1):\n",
    "        if sigma is None:\n",
    "            z = (l1+l2+L)%2\n",
    "            inv_sigma = 1 - 2*z\n",
    "        elif abs(sigma)==1:\n",
    "            inv_sigma = sigma\n",
    "        else: \n",
    "            raise(\"Please check sigma value, it should be +1 or -1\")\n",
    "        \n",
    "        if blocktype == 1 and n1 == n2 and l1 == l2:\n",
    "            feat_blocktype = inv_sigma\n",
    "                 \n",
    "        if inv_sigma == -1 and blocktype == 0 and n1 == n2 and l1 == l2:\n",
    "            continue     \n",
    "        \n",
    "        keys_L.append([(order_nu, inv_sigma, L, species1, species2, feat_blocktype)])\n",
    "    #     feat= (blocktype, L,sigma,species1, species2)\n",
    "    feat = Labels([\"order_nu\", \"inversion_sigma\", \"spherical_harmonics_l\", \"species_center\", \"species_neighbor\", \"block_type\"], np.asarray(keys_L, dtype=np.int32).reshape(-1,6))\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "a3361a26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:32:57.618899Z",
     "start_time": "2022-12-29T10:32:57.616231Z"
    }
   },
   "outputs": [],
   "source": [
    "# frames = ase.io.read(\"data/water_rotated/water_rotated_3.xyz\",\":\")\n",
    "# for f in frames:\n",
    "#     f.cell = [100,100,100]\n",
    "#     f.positions += 50\n",
    "    \n",
    "# jorbs = json.loads(json.load(open('data/water-hamiltonian/water_orbs.json', \"r\")))\n",
    "# orbs = {}\n",
    "# zdic = {\"O\" : 8, \"H\":1}\n",
    "# for k in jorbs:\n",
    "#     orbs[zdic[k]] = jorbs[k]\n",
    "# focks = np.load(\"data/water_rotated/water_rotated_saph_3.npy\", allow_pickle=True)[:len(frames)]\n",
    "# rotations = np.load(\"data/water_rotated/rotations_3.npy\", allow_pickle = True)\n",
    "# focks2 = np.load(\"data/ethanol-hamiltonian/ethanol_saph_orthogonal.npy\", allow_pickle = True)[:len(frames)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "10faaf55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:32:57.784980Z",
     "start_time": "2022-12-29T10:32:57.781315Z"
    }
   },
   "outputs": [],
   "source": [
    "def lowdin_orthogonalize(fock, s):\n",
    "    \"\"\"\n",
    "    lowdin orthogonalization of a fock matrix computing the square root of the overlap matrix\n",
    "    \"\"\"\n",
    "    eva, eve = np.linalg.eigh(s)\n",
    "    sm12 = eve @ np.diag(1.0/np.sqrt(eva)) @ eve.T\n",
    "    return sm12 @ fock @ sm12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "4b86aa3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:32:58.853267Z",
     "start_time": "2022-12-29T10:32:58.807647Z"
    }
   },
   "outputs": [],
   "source": [
    "frames1 = ase.io.read(\"data/water-hamiltonian/water_coords_1000.xyz\",\":50\")\n",
    "# frames2 = ase.io.read(\"data/ethanol-hamiltonian/ethanol_4500.xyz\", \":2\")\n",
    "frames = frames1 #+ frames2\n",
    "for f in frames:\n",
    "    f.cell = [100,100,100]\n",
    "    f.positions += 50\n",
    "jorbs = json.loads(json.load(open('data/water-hamiltonian/water_orbs.json', \"r\")))\n",
    "#jorbs = json.loads(json.load(open('data/water-hamiltonian/orbs_def2_water.json', \"r\")))\n",
    "# jorbs = json.load(open('data/ethanol-hamiltonian/orbs_saph_ethanol.json', \"r\"))\n",
    "orbs = {}\n",
    "zdic = {\"O\" : 8, \"H\":1, \"C\":6}\n",
    "for k in jorbs:\n",
    "    orbs[zdic[k]] = jorbs[k]\n",
    "# \n",
    "# focks = np.load(\"data/water-hamiltonian/water_fock.npy\", allow_pickle=True)[:len(frames1)]\n",
    "# overlap = np.load(\"data/water-hamiltonian/water_overlap.npy\", allow_pickle=True)[:len(frames1)]\n",
    "\n",
    "# orthogonal = []\n",
    "# for i in range(len(focks)): \n",
    "#     focks[i] = fix_pyscf_l1(focks[i],frames[i], orbs)\n",
    "#     overlap[i] = fix_pyscf_l1(overlap[i],frames[i], orbs)\n",
    "#     orthogonal.append(lowdin_orthogonalize(focks[i], overlap[i]))\n",
    "#focks = np.asarray(orthogonal, dtype=np.float64)\n",
    "focks1 = np.load(\"data/water-hamiltonian/water_saph_orthogonal.npy\", allow_pickle=True)[:len(frames1)]\n",
    "# focks2 = np.load(\"data/ethanol-hamiltonian/ethanol_saph_orthogonal.npy\", allow_pickle = True)[:len(frames2)]\n",
    "focks = focks1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "246e69de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:21.415280Z",
     "start_time": "2022-12-29T10:13:21.413154Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# #generating jagged matrix\n",
    "# f=[]\n",
    "# for x in focks:\n",
    "#     f.append(x)\n",
    "# for x in focks2:\n",
    "#     f.append(x)\n",
    "    \n",
    "# jagged = np.asanyarray(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "8dc18008",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:21.955173Z",
     "start_time": "2022-12-29T10:13:21.565274Z"
    }
   },
   "outputs": [],
   "source": [
    "cg = ClebschGordanReal(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ad712",
   "metadata": {},
   "source": [
    "## Manipulate Hamiltonian into blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "461e38a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:22.009450Z",
     "start_time": "2022-12-29T10:13:21.958308Z"
    }
   },
   "outputs": [],
   "source": [
    "blocks = dense_to_blocks(focks, frames, orbs)\n",
    "fock_bc = couple_blocks(blocks, cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "66014af4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:22.018261Z",
     "start_time": "2022-12-29T10:13:22.012913Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMap with 8 blocks\n",
       "keys: ['block_type' 'a_i' 'n_i' 'l_i' 'a_j' 'n_j' 'l_j' 'L']\n",
       "            0        8     2     0     8     2     0    0\n",
       "            0        8     2     0     8     2     1    1\n",
       "            0        8     2     1     8     2     1    0\n",
       "            0        8     2     1     8     2     1    2\n",
       "            2        1     1     0     8     2     0    0\n",
       "            2        1     1     0     8     2     1    1\n",
       "            0        1     1     0     1     1     0    0\n",
       "            1        1     1     0     1     1     0    0"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fock_bc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8cfe3",
   "metadata": {},
   "source": [
    "## Feature computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "13064b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:22.290636Z",
     "start_time": "2022-12-29T10:13:22.268469Z"
    }
   },
   "outputs": [],
   "source": [
    "rascal_hypers = {\n",
    "    \"interaction_cutoff\": 4.0,\n",
    "    \"cutoff_smooth_width\": 0.5,\n",
    "    \"max_radial\": 6,\n",
    "    \"max_angular\": 4,\n",
    "    \"gaussian_sigma_constant\" : 0.2,\n",
    "    \"gaussian_sigma_type\": \"Constant\",\n",
    "    \"compute_gradients\":  False,\n",
    "}\n",
    "\n",
    "spex = RascalSphericalExpansion(rascal_hypers)\n",
    "rhoi = spex.compute(frames)\n",
    "\n",
    "lmax = rascal_hypers[\"max_angular\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "cc39e55f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:22.446121Z",
     "start_time": "2022-12-29T10:13:22.422342Z"
    }
   },
   "outputs": [],
   "source": [
    "pairs = RascalPairExpansion(rascal_hypers)\n",
    "gij = pairs.compute(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "abf74c18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:22.590622Z",
     "start_time": "2022-12-29T10:13:22.570576Z"
    }
   },
   "outputs": [],
   "source": [
    "rho1i = acdc_standardize_keys(rhoi)\n",
    "rho1i.keys_to_properties(['species_neighbor'])\n",
    "gij =  acdc_standardize_keys(gij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "6a666711",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:23.085009Z",
     "start_time": "2022-12-29T10:13:22.721650Z"
    }
   },
   "outputs": [],
   "source": [
    "rho2i = cg_increment(rho1i, rho1i, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "4565f04d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:23.089623Z",
     "start_time": "2022-12-29T10:13:23.086815Z"
    }
   },
   "outputs": [],
   "source": [
    "#rho3i = cg_increment(rho2i, rho1i, lcut=2, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "bdcb6035",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:23.852396Z",
     "start_time": "2022-12-29T10:13:23.199794Z"
    }
   },
   "outputs": [],
   "source": [
    "rho1ij = cg_increment(rho1i, gij, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "0c5d9d42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:23.857723Z",
     "start_time": "2022-12-29T10:13:23.855301Z"
    }
   },
   "outputs": [],
   "source": [
    "#rho2ij = cg_increment(rho2i, gij, lcut=2, other_keys_match=[\"species_center\"], clebsch_gordan=cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "b29eb700",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:23.969106Z",
     "start_time": "2022-12-29T10:13:23.859740Z"
    }
   },
   "outputs": [],
   "source": [
    "features = hamiltonian_features(rho2i, rho1ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "7eeb7fa0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:23.975255Z",
     "start_time": "2022-12-29T10:13:23.971499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMap with 45 blocks\n",
       "keys: ['order_nu' 'inversion_sigma' 'spherical_harmonics_l' 'species_center' 'species_neighbor' 'block_type']\n",
       "           2             1                    0                   1                1              0\n",
       "           2             1                    1                   1                1              0\n",
       "           2             1                    2                   1                1              0\n",
       "        ...\n",
       "           2            -1                    4                   1                1              1\n",
       "           2            -1                    4                   1                1             -1\n",
       "           2            -1                    4                   1                8              2"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "d7bf0c77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:25.097144Z",
     "start_time": "2022-12-29T10:13:23.977349Z"
    }
   },
   "outputs": [],
   "source": [
    "from equistore.io import save\n",
    "save(\"feature.npz\", features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c7da8",
   "metadata": {},
   "source": [
    "## Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "08174c9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:25.111998Z",
     "start_time": "2022-12-29T10:13:25.101463Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_feats(feat, all_blocks=True): \n",
    "    all_norm = 0\n",
    "    normalized_blocks=[]\n",
    "    for block_idx, block in feat: \n",
    "        block_norm = np.linalg.norm(block.values)\n",
    "#         print(block_idx, block_norm)\n",
    "        all_norm = block_norm**2 * len(block.samples) \n",
    "    \n",
    "        newblock = TensorBlock(\n",
    "                        values=block.values/np.sqrt(all_norm ),\n",
    "                        samples=block.samples,\n",
    "                        components=block.components,\n",
    "                        properties= block.properties)                    \n",
    "        normalized_blocks.append(newblock) \n",
    "        \n",
    "    norm_feat = TensorMap(feat.keys, normalized_blocks)\n",
    "    raise Exception (\"Dont do it!\")\n",
    "    return norm_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "279a28b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:25.166059Z",
     "start_time": "2022-12-29T10:13:25.114906Z"
    }
   },
   "outputs": [],
   "source": [
    "#norm_feat = normalize_feats(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "e55b753b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:25.240020Z",
     "start_time": "2022-12-29T10:13:25.167741Z"
    }
   },
   "outputs": [],
   "source": [
    "# from equistore.io import save\n",
    "# save(\"./norm_feat.npz\", norm_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27106a59",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "4e4ffec4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:25.313554Z",
     "start_time": "2022-12-29T10:13:25.242815Z"
    }
   },
   "outputs": [],
   "source": [
    "from equistore.io import _labels_from_npz\n",
    "import equistore.operations as operations\n",
    "\n",
    "class HamiltonianDataset(torch.utils.data.Dataset):\n",
    "    #Dataset class\n",
    "    def __init__(self, feature_path, target, frames, feature_nu = 2):\n",
    "        #\n",
    "        self.features = np.load(feature_path, mmap_mode = 'r')\n",
    "        #self.target = np.load(target_path, mmap_mode = 'r') \n",
    "        self.target = target #Uncoupled hamiltonian \n",
    "        self.keys_features = equistore.io._labels_from_npz(self.features[\"keys\"])\n",
    "        self.currentkey = self.target.keys[0]\n",
    "        self.feature_nu = feature_nu\n",
    "        self.frames = frames\n",
    "        \n",
    "        self.allfeatkey = []\n",
    "        for t_key in self.target.keys:\n",
    "            feature_key = self.get_feature_keys(t_key)\n",
    "            self.allfeatkey.append(feature_key)\n",
    "        #Remove Duplicates\n",
    "        nodupes = set()\n",
    "        for x in self.allfeatkey:\n",
    "            if len(x) > 1:\n",
    "                for z in x:\n",
    "                    nodupes.add(tuple(z))\n",
    "            else:\n",
    "                nodupes.add(tuple(x[0]))\n",
    "        \n",
    "        nodupes = np.array(list(nodupes), np.int32)\n",
    "        \n",
    "        self.allfeatkey = Labels(names = self.allfeatkey[0].dtype.names, values = nodupes)\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "    \n",
    "    def __getitem__(self, structure_idx):\n",
    "        feature_block, feature_key = self.generate_feature_block(self.features, structure_idx)        \n",
    "        #samples_filter, target_block_samples = self.get_index_from_idx(self.target.block(self.currentkey).samples, structure_idx)\n",
    "\n",
    "        if self.currentkey is None:\n",
    "            t_blocks = []\n",
    "            for _, block in self.target:            \n",
    "                t_block = operations.slice_block(block, samples = Labels(names = ['structure'], values = (np.array(structure_idx)+1).reshape(-1,1)) )\n",
    "                t_blocks.append(t_block)\n",
    "            target_block = TensorMap(self.target.keys, t_blocks)\n",
    "        else:\n",
    "            target_block = operations.slice_block(self.target.block(self.currentkey), samples = Labels(names = ['structure'], values = (np.array(structure_idx)+1).reshape(-1,1)) )\n",
    "        structure = [self.frames[i] for i in structure_idx]\n",
    "        #Modify feature_block to tensormap\n",
    "        feature_map = TensorMap(feature_key, feature_block)\n",
    "        return feature_map, target_block, structure\n",
    "\n",
    "\n",
    "    def get_feature_keys(self,uncoupled_key):\n",
    "        return get_feat_keys_from_uncoupled(uncoupled_key, order_nu = self.feature_nu)\n",
    "    \n",
    "    def generate_feature_block(self, memmap, structure_idx):\n",
    "        #Generate the block from npz file\n",
    "        output = []\n",
    "        if self.currentkey is None:\n",
    "            feature_key = self.allfeatkey\n",
    "                \n",
    "        else:\n",
    "            feature_key = self.get_feature_keys(self.currentkey)\n",
    "            \n",
    "        for key in feature_key:\n",
    "            block_index = list(self.keys_features).index(key)\n",
    "            prefix = f\"blocks/{block_index}/values\"        \n",
    "            block_samples = equistore.io._labels_from_npz(memmap[f\"{prefix}/samples\"])\n",
    "            block_components = []\n",
    "            for i in range(1):\n",
    "                block_components.append(equistore.io._labels_from_npz(memmap[f\"{prefix}/components/{i}\"]))\n",
    "            block_properties = equistore.io._labels_from_npz(memmap[f\"{prefix}/properties\"])\n",
    "             \n",
    "\n",
    "            samples_filter, block_samples = self.get_index_from_idx(block_samples, structure_idx)\n",
    "\n",
    "            block_data = memmap[f\"{prefix}/data\"][samples_filter]\n",
    "            block = TensorBlock(block_data, block_samples, block_components, block_properties)\n",
    "            output.append(block)\n",
    "        return output, feature_key\n",
    "    \n",
    "    def get_n_properties(self, memmap, key):\n",
    "        block_index = list(self.keys_features).index(key)\n",
    "        prefix = f\"blocks/{block_index}/values\"  \n",
    "        block_properties = equistore.io._labels_from_npz(memmap[f\"{prefix}/properties\"])\n",
    "        \n",
    "        return len(block_properties)\n",
    "    \n",
    "    def get_index_from_idx(self, block_samples, structure_idx):\n",
    "        #Get samples label from IDX\n",
    "        samples = Labels(names = ['structure'], values = np.array(structure_idx).reshape(-1,1))\n",
    "        \n",
    "        all_samples = block_samples[['structure']].tolist()\n",
    "        set_samples_to_slice = set(samples.tolist())\n",
    "        samples_filter = np.array(\n",
    "            [sample in set_samples_to_slice for sample in all_samples]\n",
    "        )\n",
    "        new_samples = block_samples[samples_filter]\n",
    "        \n",
    "        return samples_filter, new_samples\n",
    "    \n",
    "    def collate_output_values(blocks):\n",
    "        feature_out = []\n",
    "        target_out = []\n",
    "        for sample_output in blocks:\n",
    "            feature_block, target_block, structure = sample_output\n",
    "            for z in feature_block:\n",
    "                feature_out.append(torch.tensor(z.values))\n",
    "            target_out.append(torch.tensor(target_block.values))\n",
    "\n",
    "        return feature_out, target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "b8ac1d81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:25.375739Z",
     "start_time": "2022-12-29T10:13:25.315749Z"
    }
   },
   "outputs": [],
   "source": [
    "test_target_path = \"./test_fock.npz\"\n",
    "test_feature_path = \"./norm_feat.npz\"\n",
    "test = HamiltonianDataset(test_feature_path, blocks, frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a0e98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90ffe2cd",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "8c336070",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:26.430883Z",
     "start_time": "2022-12-29T10:13:26.422620Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_blocks(block_tuple):\n",
    "    feature_tensor_map, target_block, structure_array = block_tuple[0]\n",
    "    \n",
    "    return feature_tensor_map, target_block, structure_array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "3c4dd925",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:26.603888Z",
     "start_time": "2022-12-29T10:13:26.600434Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, BatchSampler, SubsetRandomSampler\n",
    "\n",
    "\n",
    "#Sampler = torch.utils.data.SubsetRandomSampler(range(1,len(test)+1), generator=None)\n",
    "Sampler = torch.utils.data.sampler.RandomSampler(test)\n",
    "BSampler = torch.utils.data.sampler.BatchSampler(Sampler, batch_size = 50, drop_last = False)\n",
    "\n",
    "dataloader = DataLoader(test, sampler = BSampler, collate_fn = collate_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b704bdf2",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "79125947",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:27.868652Z",
     "start_time": "2022-12-29T10:13:27.864489Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_block_samples(t_key, feature_map):\n",
    "    f_key = get_feat_keys_from_uncoupled(t_key, None , 2)\n",
    "    ss = feature_map.block(f_key[0]).samples.copy()\n",
    "    ss[\"structure\"] = ss[\"structure\"]+1\n",
    "    \n",
    "    return ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "ee997771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:28.486995Z",
     "start_time": "2022-12-29T10:13:28.454149Z"
    }
   },
   "outputs": [],
   "source": [
    "class HamModel(torch.nn.Module):\n",
    "    #Handles prediction of entire hamiltonian and derived results\n",
    "    def __init__(self, Hamiltonian_Dataset, device, regularization=None, seed=None, layer_size=None):\n",
    "        super().__init__()\n",
    "#         self.features = features \n",
    "#         self.target = target\n",
    "        self.models = torch.nn.ModuleDict()\n",
    "        self.loss_history={}\n",
    "        self.device = device\n",
    "        self.target_keys = Hamiltonian_Dataset.target.keys\n",
    "        self.block_samples = {}\n",
    "        self.block_components = {}\n",
    "        for key in Hamiltonian_Dataset.target.keys:\n",
    "#             _block_type, _a_i, _n_i, _l_i, _a_j, _n_j, _l_j = key\n",
    "#             target_keys = Hamiltonian_Dataset.target.keys[Hamiltonian_Dataset.target.blocks_matching(\n",
    "#                 block_type = _block_type, a_i = _a_i, n_i = _n_i, l_i = _l_i, a_j = _a_j,\n",
    "#                 n_j = _n_j, l_j = _l_j)]\n",
    "            \n",
    "            #self.block_samples[str(key)] = Hamiltonian_Dataset.target.block(key).samples\n",
    "            self.block_components[str(key)] = Hamiltonian_Dataset.target.block(key).components\n",
    "        \n",
    "    \n",
    "            n_inputs = []\n",
    "            model_keys = []\n",
    "\n",
    "            feature_keys = Hamiltonian_Dataset.get_feature_keys(key)\n",
    "            for f_key in feature_keys: \n",
    "                n_features = Hamiltonian_Dataset.get_n_properties(Hamiltonian_Dataset.features, f_key)\n",
    "                n_inputs.append(n_features)\n",
    "                model_keys.append(f_key)\n",
    "                \n",
    "                \n",
    "            n_outputs = np.ones_like(n_inputs)\n",
    "                \n",
    "            self.models[str(key)] = BlockModel(cg.decouple,n_inputs, n_outputs, device, model_keys, key, seed = seed, hidden_layers = layer_size)\n",
    "        self.to(device)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #Ham model uses target keys\n",
    "        pred_blocks = []\n",
    "        for t_key in self.target_keys:\n",
    "            \n",
    "            pred = self.models[str(t_key)].forward(x) #feature_tensormap must correspond to the correct features, model returns block\n",
    "            \n",
    "            #try:\n",
    "#             print (pred.shape)\n",
    "#             print ((2 * t_key['l_i'])+1)\n",
    "#             print ((2 * t_key['l_j']) + 1)\n",
    "            pred_block = TensorBlock(\n",
    "                    values=pred.reshape((-1, (2 * t_key['l_i'])+1, (2 * t_key['l_j']) + 1, 1)), #?\n",
    "                    samples = get_block_samples(t_key, x),\n",
    "                    components = self.block_components[str(t_key)] ,\n",
    "                    properties= Labels([\"dummy\"], np.asarray([[0]], dtype=np.int32))\n",
    "                )\n",
    "#             except:\n",
    "#                 print (t_key)\n",
    "#                 print (pred)\n",
    "#                 print (self.block_samples[str(t_key)])\n",
    "#                 print (self.block_components[str(t_key)])\n",
    "                \n",
    "            pred_blocks.append(pred_block)\n",
    "        pred_hamiltonian = TensorMap(self.target_keys, pred_blocks)\n",
    "        return(pred_hamiltonian)\n",
    "    \n",
    "    #write/fix forward function for train_indiv\n",
    "    \n",
    "    def train_individual(self, train_dataloader, regularization_dict, optimizer_type, n_epochs, loss_function, lr):\n",
    "        #Iterates through the keys of self.model, then for each key we will fit self.model[key] with data[key]\n",
    "        total = len(self.models)\n",
    "        for index, t_key in enumerate(self.target_keys):\n",
    "            print (\"Now training on Block {} of {}\".format(index, total))\n",
    "            train_dataloader.dataset.currentkey = t_key\n",
    "            \n",
    "            loss_history_key = self.models[str(t_key)].fit(train_dataloader, loss_function, optimizer_type, lr, regularization_dict[str(t_key)], n_epochs)\n",
    "\n",
    "            self.loss_history[str(t_key)] = loss_history_key\n",
    "    \n",
    "    def train_collective(self, train_dataloader, regularization_dict, optimizer_type, n_epochs, loss_function, lr):\n",
    "        #for every loop through target keys, we predict the corresponding block and assemble the final hamiltonian\n",
    "        optimizer_dict = {}\n",
    "        if optimizer_type == \"Adam\":\n",
    "            for key in train_dataloader.dataset.target.keys:\n",
    "                optimizer_dict[str(key)] = torch.optim.Adam(self.models[str(key)].parameters(), lr = lr, weight_decay = regularization_dict[str(key)])\n",
    "            threshold = 200\n",
    "            scheduler_threshold = 200\n",
    "            tol = 0\n",
    "            history_step = 1000\n",
    "        \n",
    "        elif optimizer_type == \"LBFGS\":\n",
    "#             for key in train_dataloader.dataset.target.keys:\n",
    "#                 optimizer_dict[str(key)] = torch.optim.LBFGS(self.models[str(key)].parameters(), lr = lr)\n",
    "            optimizer_dict[0] = torch.optim.LBFGS(self.models.parameters(), lr = lr)\n",
    "            threshold = 30\n",
    "            scheduler_threshold = 30\n",
    "            tol = 0\n",
    "            history_step = 10                \n",
    "    \n",
    "        scheduler_dict = {}\n",
    "        scheduler_dict[0] = torch.optim.lr_scheduler.StepLR(optimizer_dict[0], scheduler_threshold, gamma = 0.5)\n",
    "#         for key in train_dataloader.dataset.target.keys:\n",
    "#             scheduler_dict[str(key)] = torch.optim.lr_scheduler.StepLR(optimizer_dict[str(key)], scheduler_threshold, gamma = 0.5)\n",
    "\n",
    "        reg_weights = torch.tensor(list(regularization_dict.values()))\n",
    "        best_state = copy.deepcopy(self.state_dict())\n",
    "        lowest_loss = torch.tensor(9999)\n",
    "        pred_loss = torch.tensor(0)\n",
    "        trigger = 0\n",
    "        loss_history = []\n",
    "        pbar = tqdm(range(n_epochs))\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch: {epoch}\")\n",
    "            pbar.set_postfix(pred_loss = pred_loss.item(), lowest_loss = lowest_loss.item(), trigger = trigger)\n",
    "            train_dataloader.dataset.currentkey = None\n",
    "            \n",
    "            for x_data, y_data, structure in train_dataloader: \n",
    "                self.collective_zg(optimizer_dict)\n",
    "                #x_data, y_data = x_data.to(self.device), y_data.to(self.device)\n",
    "                if optimizer_type == \"LBFGS\":\n",
    "                    def closure():\n",
    "                        self.collective_zg(optimizer_dict)\n",
    "                        _pred = self.forward(x_data)\n",
    "                        _pred_loss = loss_function(_pred, y_data, structure, orbs)       \n",
    "                        _pred_loss = torch.nan_to_num(_pred_loss, nan=lowest_loss.item(), posinf = lowest_loss.item(), neginf = lowest_loss.item())                          \n",
    "                        _reg_loss = self.get_regression_values(reg_weights) #Only works for 1 layer #Need to change!!\n",
    "                        _new_loss = _pred_loss + _reg_loss\n",
    "                        _new_loss.backward()\n",
    "                        return _new_loss\n",
    "                    for value in optimizer_dict.values():\n",
    "                        value.step(closure)\n",
    "#                     for param in self.parameters():\n",
    "#                         print (param.grad)\n",
    "                elif optimizer_type == \"Adam\":\n",
    "                    pred = self.forward(x_data)\n",
    "                    pred_loss = loss_function(pred, y_data, structure, orbs)  \n",
    "#                     reg_loss = torch.sum(torch.pow(self.nn.weight,2))#Only works for 1 layer\n",
    "                    new_loss = pred_loss \n",
    "                    new_loss.backward()\n",
    "                    self.collective_step(optimizer_dict)\n",
    "            with torch.no_grad():\n",
    "                current_loss = 0 \n",
    "                for x_data, y_data, structure in train_dataloader:\n",
    "                    pred = self.forward(x_data)\n",
    "                    current_loss  += loss_function(pred, y_data, structure, orbs)   #Loss should be normalized already\n",
    "                pred_loss = current_loss\n",
    "                reg_loss = self.get_regression_values(reg_weights)#Only works for 1 layer\n",
    "                new_loss = pred_loss + reg_loss\n",
    "\n",
    "                if pred_loss >100000 or (pred_loss.isnan().any()) :\n",
    "                    print (\"Optimizer shows weird behaviour, reinitializing at previous best_State\")\n",
    "                    self.load_state_dict(best_state)\n",
    "                    if optimizer_type == \"Adam\":\n",
    "                        optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = reg.item())\n",
    "                    elif optimizer_type == \"LBFGS\":\n",
    "                        optimizer = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "\n",
    "                if epoch % history_step == 1:\n",
    "                    loss_history.append(lowest_loss.item())\n",
    "                \n",
    "                if lowest_loss - new_loss > tol: #threshold to stop training             \n",
    "                    best_state = copy.deepcopy(self.state_dict())\n",
    "                    lowest_loss = new_loss \n",
    "                    trigger = 0 \n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    trigger += 1\n",
    "                    self.collective_step(scheduler_dict)\n",
    "                    if trigger > threshold:\n",
    "                        self.load_state_dict(best_state)\n",
    "                        print (\"Implemented early stopping with lowest_loss: {}\".format(lowest_loss))\n",
    "                        return loss_history\n",
    "        return loss_history\n",
    "        \n",
    "    def collective_step(self, dictionary):\n",
    "        for value in dictionary.values():\n",
    "            value.step()\n",
    "            \n",
    "    def collective_zg(self, dictionary):\n",
    "        for value in dictionary.values():\n",
    "            value.zero_grad()\n",
    "    \n",
    "    def get_regression_values(self, reg_weights):\n",
    "        output = []\n",
    "        for param in self.parameters():\n",
    "            output.append(torch.sum(torch.pow(param,2)))\n",
    "        try:\n",
    "            output = torch.sum(torch.tensor(output) * reg_weights)\n",
    "        except:\n",
    "            output = 0\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "3f59845d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:13:29.244766Z",
     "start_time": "2022-12-29T10:13:29.217612Z"
    }
   },
   "outputs": [],
   "source": [
    "class BlockModel(torch.nn.Module): #Currently only 1 model per block\n",
    "    def __init__(self, reconstruction_function, inputSize, outputSize, device, keys, target_key, seed = None, hidden_layers = None):\n",
    "        super().__init__()\n",
    "        self.reconstruction_function = reconstruction_function\n",
    "        self.inputSize = inputSize\n",
    "        self.outputSize = outputSize\n",
    "        self.device = device\n",
    "        self.keys = keys\n",
    "        self.target_key = target_key\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.initialize_model(seed)\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def initialize_model(self, seed):\n",
    "        \n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        \n",
    "        self.models = torch.nn.ModuleDict()\n",
    "        for index, key in enumerate(self.keys):\n",
    "            self.models[str(key)] = torch.nn.Linear(self.inputSize[index], self.outputSize[index], bias = False)\n",
    "        \n",
    "    def forward(self, feature_tensormap):\n",
    "        #Block model uses feature keys\n",
    "        pred_values = {}\n",
    "        for key in self.keys:\n",
    "            feature_values = feature_tensormap.block(key).values\n",
    "            d1, d2, d3 = feature_values.shape\n",
    "            L = int((d2 -1)/2)\n",
    "            pred = self.models[str(key)](torch.tensor(feature_values.reshape(d1 * d2, d3)))\n",
    "            pred = pred.reshape(d1,d2)\n",
    "            pred_values[L] = pred\n",
    "        \n",
    "        pred_block_values = self.reconstruction_function({(self.target_key['l_i'],self.target_key['l_j']) : pred_values})\n",
    "        \n",
    "\n",
    "        #pred = torch.hstack(pred_values)\n",
    "        #pred = self.reconstruction_function(pred_values)\n",
    "        return pred_block_values \n",
    "\n",
    "    \n",
    "    def fit(self,traindata_loader, loss_function, optimizer_type, lr, reg, n_epochs):\n",
    "        if optimizer_type == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = reg.item())\n",
    "            threshold = 200\n",
    "            scheduler_threshold = 50\n",
    "            tol = 0\n",
    "            history_step = 1000\n",
    "        \n",
    "        elif optimizer_type == \"LBFGS\":\n",
    "            optimizer = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "            threshold = 30\n",
    "            scheduler_threshold = 10\n",
    "            tol = 0\n",
    "            history_step = 10\n",
    "            \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, scheduler_threshold, gamma = 0.5)\n",
    "        best_state = copy.deepcopy(self.state_dict())\n",
    "        lowest_loss = torch.tensor(9999)\n",
    "        pred_loss = torch.tensor(0)\n",
    "        trigger = 0\n",
    "        loss_history = []\n",
    "        pbar = tqdm(range(n_epochs))\n",
    "        \n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch: {epoch}\")\n",
    "            pbar.set_postfix(pred_loss = pred_loss.item(), lowest_loss = lowest_loss.item(), trigger = trigger)\n",
    "            \n",
    "            for x_data, y_data, structure in traindata_loader: \n",
    "                optimizer.zero_grad()\n",
    "                #x_data, y_data = x_data.to(self.device), y_data.to(self.device)\n",
    "                if optimizer_type == \"LBFGS\":\n",
    "                    def closure():\n",
    "                        optimizer.zero_grad()\n",
    "                        _pred = self.forward(x_data)                                        \n",
    "                        _pred_loss = loss_function(_pred, y_data.values)\n",
    "                        _pred_loss = torch.nan_to_num(_pred_loss, nan=lowest_loss.item(), posinf = lowest_loss.item(), neginf = lowest_loss.item())                 \n",
    "                        _reg_loss = self.get_regression_values(reg.item()) #Only works for 1 layer\n",
    "                        _new_loss = _pred_loss + _reg_loss\n",
    "                        _new_loss.backward()\n",
    "                        return _new_loss\n",
    "                    optimizer.step(closure)\n",
    "\n",
    "                elif optimizer_type == \"Adam\":\n",
    "                    pred = self.forward(x_data)\n",
    "                    pred_loss = loss_function(pred, y_data.values)\n",
    "                    #reg_loss = self.get_regression_values(reg.item())#Only works for 1 layer\n",
    "                    new_loss = pred_loss #+ reg_loss\n",
    "                    new_loss.backward()\n",
    "\n",
    "                    optimizer.step()\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                current_loss = 0 \n",
    "                for x_data, y_data, structure in traindata_loader:\n",
    "                    pred = self.forward(x_data)\n",
    "                    current_loss  += loss_function(pred, y_data.values) #Loss should be normalized already\n",
    "                pred_loss = current_loss\n",
    "                reg_loss = self.get_regression_values(reg.item()) \n",
    "                new_loss = pred_loss + reg_loss\n",
    "                if pred_loss >100000 or (pred_loss.isnan().any()) :\n",
    "                    print (\"Optimizer shows weird behaviour, reinitializing at previous best_State\")\n",
    "                    self.load_state_dict(best_state)\n",
    "                    if optimizer_type == \"Adam\":\n",
    "                        optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = reg.item())\n",
    "                    elif optimizer_type == \"LBFGS\":\n",
    "                        optimizer = torch.optim.LBFGS(self.parameters(), lr = lr)\n",
    "\n",
    "                if epoch % history_step == 1:\n",
    "                    loss_history.append(lowest_loss.item())\n",
    "                \n",
    "                if lowest_loss - new_loss > tol: #threshold to stop training\n",
    "                    best_state = copy.deepcopy(self.state_dict())\n",
    "                    lowest_loss = new_loss \n",
    "                    trigger = 0 \n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    trigger += 1\n",
    "                    scheduler.step()\n",
    "                    if trigger > threshold:\n",
    "                        self.load_state_dict(best_state)\n",
    "                        print (\"Implemented early stopping with lowest_loss: {}\".format(lowest_loss))\n",
    "                        return loss_history\n",
    "        return loss_history\n",
    "    \n",
    "    def get_regression_values(self, reg_weights):\n",
    "        output = []\n",
    "        for param in self.parameters():\n",
    "            output.append(torch.sum(torch.pow(param,2)))\n",
    "        try:\n",
    "            output = torch.sum(torch.tensor(output) * reg_weights)\n",
    "        except:\n",
    "            output = 0\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a88f0b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "3049caf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:52:20.408855Z",
     "start_time": "2022-12-29T10:52:20.399122Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse_block_values(pred, true):\n",
    "    true = true.reshape(true.shape[:-1]) \n",
    "    MSE = torch.sum(torch.pow(true - pred,2)) / torch.numel(true)\n",
    "    return MSE*10**7\n",
    "\n",
    "def mse_full(pred_blocks, fock,frame, orbs):\n",
    "    predicted = blocks_to_dense(pred_blocks, frame, orbs)\n",
    "    #fock = torch.tensor(focks)\n",
    "    #print (mse_full_blockwise(pred_blocks, blocks, frame, orbs))\n",
    "    mse_loss = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        mse_loss[i] = ((torch.linalg.norm(fock[i]-predicted[i]))**2)/torch.numel(fock[i])\n",
    "        #print(\"from mse\", i, fock[i], mse_loss[i])\n",
    "    return torch.mean(mse_loss)*(Hartree)**2, mse_loss\n",
    "\n",
    "def mse_full_blockwise(pred_blocks, block_tensormap, frame, orbs):\n",
    "    indiv_mse = torch.zeros(1)\n",
    "    for key,block in pred_blocks:\n",
    "        #MSE = ((torch.linalg.norm(block_tensormap.block(key).values-block.values))**2)\n",
    "        print (key)\n",
    "        print (((torch.linalg.norm(block_tensormap.block(key).values- block.values))**2)/ torch.numel(block_tensormap.block(key).values))\n",
    "        print (torch.sum(torch.pow(block_tensormap.block(key).values - block.values, 2)) / torch.numel(block_tensormap.block(key).values))\n",
    "        MSE = torch.sum(torch.pow(block_tensormap.block(key).values - block.values, 2)) / torch.numel(block_tensormap.block(key).values)\n",
    "        indiv_mse += MSE\n",
    "    \n",
    "    return (indiv_mse/len(frame))*(Hartree)**2, indiv_mse\n",
    "    \n",
    "def mse_eigvals(pred_blocks, fock, frame, orbs):\n",
    "    fock = torch.tensor(focks)\n",
    "    predicted = blocks_to_dense(pred_blocks, frame, orbs)\n",
    "    evanorm = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        evanorm[i] = torch.mean((torch.linalg.eigvalsh(fock[i]) - torch.linalg.eigvalsh(predicted[i]))**2)/len(fock[i])\n",
    "    return torch.mean(evanorm)*(Hartree)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "548dd259",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:46:18.959375Z",
     "start_time": "2022-12-29T10:46:18.941861Z"
    }
   },
   "outputs": [],
   "source": [
    "blocks_zero = dense_to_blocks(np.zeros_like(focks), frames, orbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "ad62eaad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:46:29.867773Z",
     "start_time": "2022-12-29T10:46:29.828883Z"
    }
   },
   "outputs": [],
   "source": [
    "blocks_ones = dense_to_blocks(np.ones_like(focks), frames, orbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "8d951a91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:52:22.694210Z",
     "start_time": "2022-12-29T10:52:22.652454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(740.4595)\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a,  b = mse_full(blocks_ones,torch.zeros_like(torch.tensor(focks)), frames, orbs)\n",
    "print (a)\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "76a5a3de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:55:10.915845Z",
     "start_time": "2022-12-29T10:55:10.909248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Labels([(0, 8, 2, 0, 8, 2, 0), (0, 8, 2, 0, 8, 2, 1),\n",
       "        (0, 8, 2, 1, 8, 2, 1), (2, 1, 1, 0, 8, 2, 0),\n",
       "        (2, 1, 1, 0, 8, 2, 1), (0, 1, 1, 0, 1, 1, 0),\n",
       "        (1, 1, 1, 0, 1, 1, 0)],\n",
       "       dtype=[('block_type', '<i4'), ('a_i', '<i4'), ('n_i', '<i4'), ('l_i', '<i4'), ('a_j', '<i4'), ('n_j', '<i4'), ('l_j', '<i4')])"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks_ones.keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "1e61008a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:51:19.296719Z",
     "start_time": "2022-12-29T10:51:19.287776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1, 1, 1])"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks_zero.block(6).values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "b6b952bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:28:48.857055Z",
     "start_time": "2022-12-29T10:28:40.887655Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|| 4/4 [00:07<00:00,  1.98s/it, lowest_loss=0.0556, pred_loss=0.0556, trigger=0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0267622572189157]"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testham = HamModel(test, \"cpu\")\n",
    "regularization_dict = {}\n",
    "# for a in range (16):\n",
    "#     regularization_dict[a] = torch.tensor(0)\n",
    "# for value in testham.models.values():\n",
    "#     for key in value.models:\n",
    "\n",
    "#         regularization_dict[str(key)] = torch.tensor(0)\n",
    "# print (len(regularization_dict))\n",
    "\n",
    "for key in blocks.keys:\n",
    "    regularization_dict[str(key)] = torch.tensor(0)\n",
    "#testham.train_individual(dataloader, regularization_dict, \"LBFGS\", 1000, mse_block_values, 1)\n",
    "testham.train_collective(dataloader, regularization_dict, \"LBFGS\", 4, mse_full, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "600633a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:28:51.422183Z",
     "start_time": "2022-12-29T10:28:49.566361Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|| 4/4 [00:01<00:00,  2.20it/s, lowest_loss=0.0416, pred_loss=0.0416, trigger=0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4445156106576071]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testham2 = HamModel(test, \"cpu\")\n",
    "regularization_dict = {}\n",
    "# for a in range (16):\n",
    "#     regularization_dict[a] = torch.tensor(0)\n",
    "# for value in testham.models.values():\n",
    "#     for key in value.models:\n",
    "\n",
    "#         regularization_dict[str(key)] = torch.tensor(0)\n",
    "# print (len(regularization_dict))\n",
    "\n",
    "for key in blocks.keys:\n",
    "    regularization_dict[str(key)] = torch.tensor(0)\n",
    "#testham.train_individual(dataloader, regularization_dict, \"LBFGS\", 1000, mse_block_values, 1)\n",
    "testham2.train_collective(dataloader, regularization_dict, \"LBFGS\", 4, mse_full_blockwise, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "caed8c7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:18:41.558303Z",
     "start_time": "2022-12-29T10:18:23.956975Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training on Block 0 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 40:  80%|                     | 40/50 [00:01<00:00, 36.60it/s, lowest_loss=0.000869, pred_loss=0.000869, trigger=30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implemented early stopping with lowest_loss: 0.0008685962322613775\n",
      "Now training on Block 1 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 41:  82%|                    | 41/50 [00:01<00:00, 26.76it/s, lowest_loss=0.00139, pred_loss=0.00139, trigger=30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implemented early stopping with lowest_loss: 0.0013860750660304058\n",
      "Now training on Block 2 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 49: 100%|| 50/50 [00:06<00:00,  7.48it/s, lowest_loss=0.0275, pred_loss=0.0275, trigger=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training on Block 3 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 48:  96%|    | 48/50 [00:01<00:00, 30.51it/s, lowest_loss=0.00264, pred_loss=0.00264, trigger=30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implemented early stopping with lowest_loss: 0.0026448409589528953\n",
      "Now training on Block 4 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 49: 100%|| 50/50 [00:03<00:00, 12.61it/s, lowest_loss=0.0269, pred_loss=0.0269, trigger=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training on Block 5 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 47:  94%|      | 47/50 [00:01<00:00, 28.64it/s, lowest_loss=0.0333, pred_loss=0.0333, trigger=30]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implemented early stopping with lowest_loss: 0.033257556093282035\n",
      "Now training on Block 6 of 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 40:  80%|                     | 40/50 [00:01<00:00, 38.05it/s, lowest_loss=0.000327, pred_loss=0.000327, trigger=30]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implemented early stopping with lowest_loss: 0.0003266998871763507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "regularization_dict = {}\n",
    "for key in blocks.keys:\n",
    "    regularization_dict[str(key)] = torch.tensor(0)\n",
    "testham = HamModel(test, \"cpu\")\n",
    "testham.train_individual(dataloader, regularization_dict, \"LBFGS\", 50, mse_block_values, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "f1080c3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:01.235301Z",
     "start_time": "2022-12-29T10:15:01.225690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorMap with 7 blocks\n",
       "keys: ['block_type' 'a_i' 'n_i' 'l_i' 'a_j' 'n_j' 'l_j']\n",
       "            0        8     2     0     8     2     0\n",
       "            0        8     2     0     8     2     1\n",
       "            0        8     2     1     8     2     1\n",
       "            2        1     1     0     8     2     0\n",
       "            2        1     1     0     8     2     1\n",
       "            0        1     1     0     1     1     0\n",
       "            1        1     1     0     1     1     0"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5122692",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "eb10f943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:53.043094Z",
     "start_time": "2022-12-29T10:15:52.986380Z"
    }
   },
   "outputs": [],
   "source": [
    "#Load test set\n",
    "test_frames1 = ase.io.read(\"data/water-hamiltonian/water_coords_1000.xyz\",\"50:80\")\n",
    "# frames2 = ase.io.read(\"data/ethanol-hamiltonian/ethanol_4500.xyz\", \":2\")\n",
    "test_frames = test_frames1 #+ frames2\n",
    "for f in test_frames:\n",
    "    f.cell = [100,100,100]\n",
    "    f.positions += 50\n",
    "\n",
    "# test_focks1 = np.load(\"data/water-hamiltonian/water_saph_orthogonal.npy\", allow_pickle=True)[50:60]\n",
    "# focks2 = np.load(\"data/ethanol-hamiltonian/ethanol_saph_orthogonal.npy\", allow_pickle = True)[:len(frames2)]\n",
    "\n",
    "test_focks1 = np.load(\"data/water-hamiltonian/water_saph_orthogonal.npy\", allow_pickle=True)[50:80]\n",
    "test_focks = test_focks1\n",
    "# test_focks = np.load(\"data/water-hamiltonian/water_fock.npy\", allow_pickle=True)[50:80]\n",
    "# test_overlap = np.load(\"data/water-hamiltonian/water_overlap.npy\", allow_pickle=True)[50:80]\n",
    "\n",
    "# test_orthogonal = []\n",
    "# for i in range(len(test_focks)): \n",
    "#     test_focks[i] = fix_pyscf_l1(test_focks[i],test_frames[i], orbs)\n",
    "#     test_overlap[i] = fix_pyscf_l1(test_overlap[i],test_frames[i], orbs)\n",
    "#     test_orthogonal.append(lowdin_orthogonalize(test_focks[i], test_overlap[i]))\n",
    "# test_focks = np.asarray(test_orthogonal, dtype=np.float64)\n",
    "    \n",
    "test_blocks = dense_to_blocks(test_focks, test_frames, orbs)\n",
    "test_fock_bc = couple_blocks(test_blocks, cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "515d2062",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:54.725559Z",
     "start_time": "2022-12-29T10:15:53.932535Z"
    }
   },
   "outputs": [],
   "source": [
    "test_rhoi = spex.compute(test_frames)\n",
    "test_gij = pairs.compute(test_frames)\n",
    "test_rho1i = acdc_standardize_keys(test_rhoi)\n",
    "test_rho1i.keys_to_properties(['species_neighbor'])\n",
    "test_gij =  acdc_standardize_keys(test_gij)\n",
    "test_rho2i = cg_increment(test_rho1i, test_rho1i, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)\n",
    "test_rho1ij = cg_increment(test_rho1i, test_gij, lcut=lmax, other_keys_match=[\"species_center\"], clebsch_gordan=cg)\n",
    "\n",
    "test_features = hamiltonian_features(test_rho2i, test_rho1ij)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "695e248b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:55.390807Z",
     "start_time": "2022-12-29T10:15:54.727705Z"
    }
   },
   "outputs": [],
   "source": [
    "from equistore.io import save\n",
    "save(\"test_feature.npz\", test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "514168bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:55.398022Z",
     "start_time": "2022-12-29T10:15:55.394196Z"
    }
   },
   "outputs": [],
   "source": [
    "# norm_test_feat = normalize_feats(test_features)\n",
    "# from equistore.io import save\n",
    "# save(\"./norm_testfeat.npz\", norm_test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d4e81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T16:26:03.738575Z",
     "start_time": "2022-12-28T16:26:03.164926Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "f5ee3635",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:55.442916Z",
     "start_time": "2022-12-29T10:15:55.402080Z"
    }
   },
   "outputs": [],
   "source": [
    "#test_target_path = \"./test_fock.npz\"\n",
    "test_feature_path = \"./test_feature.npz\"\n",
    "testing = HamiltonianDataset(test_feature_path, test_blocks, test_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "1e979a9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:55.726557Z",
     "start_time": "2022-12-29T10:15:55.722442Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, BatchSampler, SubsetRandomSampler\n",
    "\n",
    "\n",
    "#Sampler = torch.utils.data.SubsetRandomSampler(range(1,len(test)+1), generator=None)\n",
    "test_Sampler = torch.utils.data.sampler.RandomSampler(testing)\n",
    "test_BSampler = torch.utils.data.sampler.BatchSampler(test_Sampler, batch_size = 50, drop_last = False)\n",
    "\n",
    "test_dataloader = DataLoader(testing, sampler = test_BSampler, collate_fn = collate_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "1091e288",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:25:09.307646Z",
     "start_time": "2022-12-29T10:25:09.295845Z"
    }
   },
   "outputs": [],
   "source": [
    "def mse_block_values(pred, true):\n",
    "    true = true.reshape(true.shape[:-1])\n",
    "    MSE = torch.sum(torch.pow(true - pred,2)) / torch.numel(true)\n",
    "    return MSE\n",
    "\n",
    "def mse_full(pred_blocks, fock,frame, orbs):\n",
    "    predicted = blocks_to_dense(pred_blocks, frame, orbs)\n",
    "    #fock = torch.tensor(focks)\n",
    "    mse_loss = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        mse_loss[i] = ((torch.linalg.norm(fock[i]-predicted[i]))**2)#/torch.numel(fock[i])\n",
    "\n",
    "        #print(\"from mse\", i, fock[i], mse_loss[i])\n",
    "    return torch.mean(mse_loss)*(Hartree)**2\n",
    "\n",
    "def mse_eigvals(pred_blocks, fock, frame, orbs):\n",
    "    #fock = torch.tensor(focks)\n",
    "    predicted = blocks_to_dense(pred_blocks, frame, orbs)\n",
    "    evanorm = torch.empty(len(frame))\n",
    "    for i in range(len(frame)):\n",
    "        evanorm[i] = torch.mean((torch.linalg.eigvalsh(fock[i]) - torch.linalg.eigvalsh(predicted[i]))**2)/len(fock[i])\n",
    "    return torch.mean(evanorm)*(Hartree)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "31e5f4b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:29:34.344107Z",
     "start_time": "2022-12-29T10:29:34.230337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0226, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dataloader.dataset.currentkey = None\n",
    "for x_data, y_data, structures in dataloader:\n",
    "    t_pred = testham(x_data)\n",
    "    print (mse_full(t_pred, torch.tensor(focks), structures, orbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "640612f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:29:34.489260Z",
     "start_time": "2022-12-29T10:29:34.383956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0348, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dataloader.dataset.currentkey = None\n",
    "for x_data, y_data, structures in dataloader:\n",
    "    t_pred = testham2(x_data)\n",
    "    print (mse_full(t_pred, torch.tensor(focks), structures, orbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "eaa51a8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:29:35.394632Z",
     "start_time": "2022-12-29T10:29:35.334657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0165], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dataloader.dataset.currentkey = None\n",
    "for x_data, y_data, structures in dataloader:\n",
    "    t_pred = testham(x_data)\n",
    "    print (mse_full_blockwise(t_pred, test.target, structures, orbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "7dad9d16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:29:35.528455Z",
     "start_time": "2022-12-29T10:29:35.458227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0224], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dataloader.dataset.currentkey = None\n",
    "for x_data, y_data, structures in dataloader:\n",
    "    t_pred = testham2(x_data)\n",
    "    print (mse_full_blockwise(t_pred, test.target, structures, orbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "946ec8b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:19:13.007214Z",
     "start_time": "2022-12-29T10:19:12.930342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(57.9591, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_dataloader.dataset.currentkey = None\n",
    "for x_data, y_data, structures in test_dataloader:\n",
    "    pred = testham(x_data)\n",
    "    print (mse_full(pred, torch.tensor(test_focks), structures, orbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "08a1fc60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:17:34.503384Z",
     "start_time": "2022-12-29T10:17:34.418599Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'block'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [371]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m x_data, y_data, structures \u001b[38;5;241m=\u001b[39m testing[[a \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m)]]\n\u001b[1;32m      3\u001b[0m pred \u001b[38;5;241m=\u001b[39m testham(x_data)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[43mmse_full_blockwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_focks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morbs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[0;32mIn [365]\u001b[0m, in \u001b[0;36mmse_full_blockwise\u001b[0;34m(pred_blocks, block_tensormap, frame, orbs)\u001b[0m\n\u001b[1;32m     16\u001b[0m indiv_mse \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key,block \u001b[38;5;129;01min\u001b[39;00m pred_blocks:\n\u001b[0;32m---> 18\u001b[0m     MSE \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mpow(\u001b[43mblock_tensormap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m(key)\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m-\u001b[39m block\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m2\u001b[39m)) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mnumel(block_tensormap\u001b[38;5;241m.\u001b[39mblock(key)\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     19\u001b[0m     indiv_mse \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m MSE\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indiv_mse\u001b[38;5;241m*\u001b[39m(Hartree)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'block'"
     ]
    }
   ],
   "source": [
    "testing.currentkey = None\n",
    "x_data, y_data, structures = testing[[a for a in range(30)]]\n",
    "pred = testham(x_data)\n",
    "print (mse_full(pred, torch.tensor(test_focks), test_frames, orbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "61ddb3e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-29T10:15:13.390884Z",
     "start_time": "2022-12-29T10:15:13.310355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(57.3932, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test.currentkey = None\n",
    "x_data, y_data, structures = test[[a for a in range(1)]]\n",
    "t_pred = testham(x_data)\n",
    "print (mse_full(t_pred, torch.tensor(focks), frames, orbs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8660464f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T16:00:33.837518Z",
     "start_time": "2022-12-28T16:00:33.826315Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted = blocks_to_dense(pred, test_frames, orbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4ac94f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T16:02:29.277438Z",
     "start_time": "2022-12-28T16:02:29.231031Z"
    }
   },
   "outputs": [],
   "source": [
    "t_predicted = blocks_to_dense(t_pred, structures, orbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b99d9",
   "metadata": {},
   "source": [
    "## Tests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4dd3b",
   "metadata": {},
   "source": [
    "### set up wigner-d rotations matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7bec7ccf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T15:59:27.166039Z",
     "start_time": "2022-12-28T15:59:27.094891Z"
    },
    "hide_input": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rotations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [57]\u001b[0m, in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m _wddict2 \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m L \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, lmax \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 35\u001b[0m     wig1 \u001b[38;5;241m=\u001b[39m wigner_d(L, \u001b[38;5;241m*\u001b[39m\u001b[43mrotations\u001b[49m[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     36\u001b[0m     wig2 \u001b[38;5;241m=\u001b[39m wigner_d(L, \u001b[38;5;241m*\u001b[39mrotations[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     37\u001b[0m     _wddict1[L] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreal(c2r_mats[L] \u001b[38;5;241m@\u001b[39m np\u001b[38;5;241m.\u001b[39mconjugate(wig1) \u001b[38;5;241m@\u001b[39m r2c_mats[L])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rotations' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from rascal.utils.cg_utils import _wigner_d as wigner_d\n",
    "lmax=3\n",
    "I_SQRT_2 = 1.0 / np.sqrt(2)\n",
    "SQRT_2 = np.sqrt(2)\n",
    "def _r2c(sp):\n",
    "    \"\"\"Real to complex SPH. Assumes a block with 2l+1 reals corresponding\n",
    "    to real SPH with m indices from -l to +l\"\"\"\n",
    "    l = (len(sp) - 1) // 2  # infers l from the vector size\n",
    "    rc = np.zeros(len(sp), dtype=np.complex128)\n",
    "    rc[l] = sp[l]\n",
    "    for m in range(1, l + 1):\n",
    "        rc[l + m] = (sp[l + m] + 1j * sp[l - m]) * I_SQRT_2 * (-1) ** m\n",
    "        rc[l - m] = (sp[l + m] - 1j * sp[l - m]) * I_SQRT_2\n",
    "    return rc\n",
    "def _c2r(cp):\n",
    "    \"\"\"Complex to real SPH. Assumes a block with 2l+1 complex\n",
    "    corresponding to Y^m_l with m indices from -l to +l\"\"\"\n",
    "    l = (len(cp) - 1) // 2  # infers l from the vector size\n",
    "    rs = np.zeros(len(cp), dtype=np.float64)\n",
    "    rs[l] = np.real(cp[l])\n",
    "    for m in range(1, l + 1):\n",
    "        rs[l - m] = (-1) ** m * SQRT_2 * np.imag(cp[l + m])\n",
    "        rs[l + m] = (-1) ** m * SQRT_2 * np.real(cp[l + m])\n",
    "    return rs\n",
    "r2c_mats = {}\n",
    "c2r_mats = {}\n",
    "for L in range(0, lmax + 1):\n",
    "    r2c_mats[L] = np.hstack(\n",
    "        [_r2c(np.eye(2 * L + 1)[i])[:, np.newaxis] for i in range(2 * L + 1)]\n",
    "    )\n",
    "    c2r_mats[L] = np.conjugate(r2c_mats[L]).T\n",
    "_wddict1 = {}\n",
    "_wddict2 = {}\n",
    "for L in range(0, lmax + 1):\n",
    "    wig1 = wigner_d(L, *rotations[1])\n",
    "    wig2 = wigner_d(L, *rotations[2])\n",
    "    _wddict1[L] = np.real(c2r_mats[L] @ np.conjugate(wig1) @ r2c_mats[L])\n",
    "    _wddict2[L] = np.real(c2r_mats[L] @ np.conjugate(wig2) @ r2c_mats[L])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3456b5",
   "metadata": {},
   "source": [
    "### block wise rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a9e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "cg_coefs_list = []\n",
    "pred_block_list = []\n",
    "for key in testham.target_keys:\n",
    "    test.currentkey = key\n",
    "    x_data, y_data, structures = test[0,1,2]\n",
    "    cg_coefs, pred_block = testham.models[str(key)].get_cg_coefs(x_data)\n",
    "    cg_coefs_list.append(cg_coefs)\n",
    "    pred_block_list.append(pred_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "7680ec54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T13:52:33.506296Z",
     "start_time": "2022-12-28T13:52:33.499610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2676e-11, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.norm( torch.tensor(_wddict1[1]) @ cg_coefs_list[1][1][0] - cg_coefs_list[1][1][1])\n",
    "torch.linalg.norm( torch.tensor(_wddict2[1]) @ cg_coefs_list[1][1][0] - cg_coefs_list[1][1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "feb96ee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T13:54:04.383037Z",
     "start_time": "2022-12-28T13:54:04.369327Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.0514e-11, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.norm( torch.tensor(_wddict1[2]) @ cg_coefs_list[2][2][0] - cg_coefs_list[2][2][1])\n",
    "torch.linalg.norm( torch.tensor(_wddict2[2]) @ cg_coefs_list[2][2][0] - cg_coefs_list[2][2][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "98029618",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T13:54:15.183312Z",
     "start_time": "2022-12-28T13:54:15.174516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: tensor([[-0.0088],\n",
       "          [-0.0088],\n",
       "          [-0.0088]], grad_fn=<ReshapeAliasBackward0>)},\n",
       " {1: tensor([[ 0.0000, -0.0057,  0.0000],\n",
       "          [-0.0026, -0.0049, -0.0012],\n",
       "          [-0.0009, -0.0029,  0.0048]], grad_fn=<ReshapeAliasBackward0>)},\n",
       " {0: tensor([[0.0085],\n",
       "          [0.0085],\n",
       "          [0.0085]], grad_fn=<ReshapeAliasBackward0>),\n",
       "  2: tensor([[ 0.0000,  0.0000,  0.0005,  0.0000, -0.0056],\n",
       "          [ 0.0050, -0.0006,  0.0013, -0.0022, -0.0003],\n",
       "          [ 0.0005, -0.0007, -0.0036, -0.0030, -0.0031]],\n",
       "         grad_fn=<ReshapeAliasBackward0>)},\n",
       " {0: tensor([[-0.0022],\n",
       "          [-0.0022],\n",
       "          [-0.0022],\n",
       "          [-0.0022],\n",
       "          [-0.0022],\n",
       "          [-0.0022]], grad_fn=<ReshapeAliasBackward0>)},\n",
       " {1: tensor([[-0.0031,  0.0021,  0.0000],\n",
       "          [ 0.0031,  0.0021,  0.0000],\n",
       "          [ 0.0028,  0.0003,  0.0024],\n",
       "          [-0.0009,  0.0033, -0.0016],\n",
       "          [ 0.0034,  0.0007, -0.0014],\n",
       "          [-0.0027,  0.0014, -0.0022]], grad_fn=<ReshapeAliasBackward0>)},\n",
       " {0: tensor([[-0.0062],\n",
       "          [-0.0062],\n",
       "          [-0.0062],\n",
       "          [-0.0062],\n",
       "          [-0.0062],\n",
       "          [-0.0062]], grad_fn=<ReshapeAliasBackward0>)},\n",
       " {0: tensor([[-0.0017],\n",
       "          [-0.0017],\n",
       "          [-0.0017]], grad_fn=<ReshapeAliasBackward0>)}]"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cg_coefs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d818d0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e412b3b2",
   "metadata": {},
   "source": [
    "### Rotate matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87821f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted_np = [i.detach().numpy() for i in predicted]\n",
    "\n",
    "pred_bc = couple_blocks(dense_to_blocks(predicted_np, frames, orbs), cg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9eec32",
   "metadata": {},
   "source": [
    "### Eigenvalue tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "980cb211",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T14:04:12.334839Z",
     "start_time": "2022-12-28T14:04:12.287143Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3136, -0.6966, -0.5671, -0.4974,  0.1744,  0.2537])\n",
      "tensor([-1.3136, -0.6966, -0.5671, -0.4974,  0.1744,  0.2537])\n",
      "tensor([-1.3136, -0.6966, -0.5671, -0.4974,  0.1744,  0.2537])\n",
      "tensor([-1.2385e-04, -9.3104e-05, -8.7328e-05, -5.9675e-05, -9.9897e-06,\n",
      "        -2.1660e-06], grad_fn=<LinalgEighBackward0>)\n",
      "tensor([-1.2385e-04, -9.3104e-05, -8.7328e-05, -5.9675e-05, -9.9897e-06,\n",
      "        -2.1660e-06], grad_fn=<LinalgEighBackward0>)\n",
      "tensor([-1.2385e-04, -9.3104e-05, -8.7328e-05, -5.9675e-05, -9.9897e-06,\n",
      "        -2.1660e-06], grad_fn=<LinalgEighBackward0>)\n"
     ]
    }
   ],
   "source": [
    "testham.currentkey = None\n",
    "x_data, y_data, structures = test[0,1,2]\n",
    "pred = testham(x_data)\n",
    "predicted = blocks_to_dense(pred, frames, orbs)\n",
    "\n",
    "\n",
    "for i in blocks_to_dense(blocks, frames, orbs):\n",
    "    print (torch.linalg.eigvalsh(i))\n",
    "\n",
    "np.linalg.eigvalsh(focks)\n",
    "\n",
    "for i in predicted:\n",
    "    print (torch.linalg.eigvalsh(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "5a5aa2a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T13:18:16.969392Z",
     "start_time": "2022-12-28T13:18:16.956309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.3136, -0.6966, -0.5671, -0.4974,  0.1744,  0.2537])\n",
      "tensor([-1.3136, -0.6966, -0.5671, -0.4974,  0.1744,  0.2537])\n",
      "tensor([-1.3136, -0.6966, -0.5671, -0.4974,  0.1744,  0.2537])\n",
      "tensor([-2.4574, -1.6094, -0.6429, -0.3724, -0.3137,  0.5011],\n",
      "       grad_fn=<LinalgEighBackward0>)\n",
      "tensor([-2.4574, -1.6094, -0.6428, -0.3724, -0.3137,  0.5012],\n",
      "       grad_fn=<LinalgEighBackward0>)\n",
      "tensor([-2.4574, -1.6094, -0.6429, -0.3724, -0.3137,  0.5011],\n",
      "       grad_fn=<LinalgEighBackward0>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25081ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4ca95d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dcd9f9e",
   "metadata": {},
   "source": [
    "### check decoupling of blocks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "85540cac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T17:39:07.856236Z",
     "start_time": "2022-12-27T17:39:07.848203Z"
    }
   },
   "outputs": [],
   "source": [
    "couple={}\n",
    "couple[0] = fock_bc.block(2).values[0].swapaxes(-2,-1)\n",
    "couple[2] = fock_bc.block(3).values[0].swapaxes(-2,-1)\n",
    "\n",
    "decoupled = cg.decouple( {(1,1):couple})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "6ee03ca5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T17:45:46.153145Z",
     "start_time": "2022-12-27T17:45:46.140403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.207001048246386e-14"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm( torch.tensor(_wddict[1]) @ fock_bc.block(1).values[0] - fock_bc.block(1).values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e670e",
   "metadata": {},
   "source": [
    "### check feature rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "f98b0c83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T14:42:14.230226Z",
     "start_time": "2022-12-28T14:42:14.217015Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.91781977311388e-09"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm( _wddict[1] @ features.block(1).values[0] - features.block(1).values[2]) / np.linalg.norm(features.block(1).values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1d56e7ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T16:51:23.961932Z",
     "start_time": "2022-12-28T16:51:23.943468Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_wddict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [79]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm( \u001b[43m_wddict\u001b[49m[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m@\u001b[39m norm_feat\u001b[38;5;241m.\u001b[39mblock(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m norm_feat\u001b[38;5;241m.\u001b[39mblock(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(norm_feat\u001b[38;5;241m.\u001b[39mblock(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name '_wddict' is not defined"
     ]
    }
   ],
   "source": [
    "np.linalg.norm( _wddict[1] @ norm_feat.block(1).values[0] - norm_feat.block(1).values[2])/np.linalg.norm(norm_feat.block(1).values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "05da5a49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T15:08:53.922644Z",
     "start_time": "2022-12-28T15:08:53.913066Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Labels([( 0, 1, 1), ( 0, 2, 2), ( 1, 1, 1), ( 1, 2, 2), ( 2, 1, 1),\n",
       "        ( 2, 2, 2), ( 3, 1, 1), ( 3, 2, 2), ( 4, 1, 1), ( 4, 2, 2),\n",
       "        ( 5, 1, 1), ( 5, 2, 2), ( 6, 1, 1), ( 6, 2, 2), ( 7, 1, 1),\n",
       "        ( 7, 2, 2), ( 8, 1, 1), ( 8, 2, 2), ( 9, 1, 1), ( 9, 2, 2),\n",
       "        (10, 1, 1), (10, 2, 2), (11, 1, 1), (11, 2, 2), (12, 1, 1),\n",
       "        (12, 2, 2), (13, 1, 1), (13, 2, 2), (14, 1, 1), (14, 2, 2),\n",
       "        (15, 1, 1), (15, 2, 2), (16, 1, 1), (16, 2, 2), (17, 1, 1),\n",
       "        (17, 2, 2), (18, 1, 1), (18, 2, 2), (19, 1, 1), (19, 2, 2),\n",
       "        (20, 1, 1), (20, 2, 2), (21, 1, 1), (21, 2, 2), (22, 1, 1),\n",
       "        (22, 2, 2), (23, 1, 1), (23, 2, 2), (24, 1, 1), (24, 2, 2),\n",
       "        (25, 1, 1), (25, 2, 2), (26, 1, 1), (26, 2, 2), (27, 1, 1),\n",
       "        (27, 2, 2), (28, 1, 1), (28, 2, 2), (29, 1, 1), (29, 2, 2),\n",
       "        (30, 1, 1), (30, 2, 2), (31, 1, 1), (31, 2, 2), (32, 1, 1),\n",
       "        (32, 2, 2), (33, 1, 1), (33, 2, 2), (34, 1, 1), (34, 2, 2),\n",
       "        (35, 1, 1), (35, 2, 2), (36, 1, 1), (36, 2, 2), (37, 1, 1),\n",
       "        (37, 2, 2), (38, 1, 1), (38, 2, 2), (39, 1, 1), (39, 2, 2),\n",
       "        (40, 1, 1), (40, 2, 2), (41, 1, 1), (41, 2, 2), (42, 1, 1),\n",
       "        (42, 2, 2), (43, 1, 1), (43, 2, 2), (44, 1, 1), (44, 2, 2),\n",
       "        (45, 1, 1), (45, 2, 2), (46, 1, 1), (46, 2, 2), (47, 1, 1),\n",
       "        (47, 2, 2), (48, 1, 1), (48, 2, 2), (49, 1, 1), (49, 2, 2)],\n",
       "       dtype=[('structure', '<i4'), ('center', '<i4'), ('neighbor', '<i4')])"
      ]
     },
     "execution_count": 664,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.block(0).samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "id": "a293041d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T15:11:09.560205Z",
     "start_time": "2022-12-28T15:11:09.548451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Labels([( 1, 1, 1), ( 1, 2, 2), ( 2, 1, 1), ( 2, 2, 2), ( 3, 1, 1),\n",
       "        ( 3, 2, 2), ( 4, 1, 1), ( 4, 2, 2), ( 5, 1, 1), ( 5, 2, 2),\n",
       "        ( 6, 1, 1), ( 6, 2, 2), ( 7, 1, 1), ( 7, 2, 2), ( 8, 1, 1),\n",
       "        ( 8, 2, 2), ( 9, 1, 1), ( 9, 2, 2), (10, 1, 1), (10, 2, 2),\n",
       "        (11, 1, 1), (11, 2, 2), (12, 1, 1), (12, 2, 2), (13, 1, 1),\n",
       "        (13, 2, 2), (14, 1, 1), (14, 2, 2), (15, 1, 1), (15, 2, 2),\n",
       "        (16, 1, 1), (16, 2, 2), (17, 1, 1), (17, 2, 2), (18, 1, 1),\n",
       "        (18, 2, 2), (19, 1, 1), (19, 2, 2), (20, 1, 1), (20, 2, 2),\n",
       "        (21, 1, 1), (21, 2, 2), (22, 1, 1), (22, 2, 2), (23, 1, 1),\n",
       "        (23, 2, 2), (24, 1, 1), (24, 2, 2), (25, 1, 1), (25, 2, 2),\n",
       "        (26, 1, 1), (26, 2, 2), (27, 1, 1), (27, 2, 2), (28, 1, 1),\n",
       "        (28, 2, 2), (29, 1, 1), (29, 2, 2), (30, 1, 1), (30, 2, 2),\n",
       "        (31, 1, 1), (31, 2, 2), (32, 1, 1), (32, 2, 2), (33, 1, 1),\n",
       "        (33, 2, 2), (34, 1, 1), (34, 2, 2), (35, 1, 1), (35, 2, 2),\n",
       "        (36, 1, 1), (36, 2, 2), (37, 1, 1), (37, 2, 2), (38, 1, 1),\n",
       "        (38, 2, 2), (39, 1, 1), (39, 2, 2), (40, 1, 1), (40, 2, 2),\n",
       "        (41, 1, 1), (41, 2, 2), (42, 1, 1), (42, 2, 2), (43, 1, 1),\n",
       "        (43, 2, 2), (44, 1, 1), (44, 2, 2), (45, 1, 1), (45, 2, 2),\n",
       "        (46, 1, 1), (46, 2, 2), (47, 1, 1), (47, 2, 2), (48, 1, 1),\n",
       "        (48, 2, 2), (49, 1, 1), (49, 2, 2), (50, 1, 1), (50, 2, 2)],\n",
       "       dtype=[('structure', '<i4'), ('center', '<i4'), ('neighbor', '<i4')])"
      ]
     },
     "execution_count": 673,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks.block(5).samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "id": "e7ae995c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T15:10:44.580485Z",
     "start_time": "2022-12-28T15:10:44.567403Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Labels([( 1, 1, 1), ( 1, 2, 2), ( 2, 1, 1), ( 2, 2, 2), ( 3, 1, 1),\n",
       "        ( 3, 2, 2), ( 4, 1, 1), ( 4, 2, 2), ( 5, 1, 1), ( 5, 2, 2),\n",
       "        ( 6, 1, 1), ( 6, 2, 2), ( 7, 1, 1), ( 7, 2, 2), ( 8, 1, 1),\n",
       "        ( 8, 2, 2), ( 9, 1, 1), ( 9, 2, 2), (10, 1, 1), (10, 2, 2),\n",
       "        (11, 1, 1), (11, 2, 2), (12, 1, 1), (12, 2, 2), (13, 1, 1),\n",
       "        (13, 2, 2), (14, 1, 1), (14, 2, 2), (15, 1, 1), (15, 2, 2),\n",
       "        (16, 1, 1), (16, 2, 2), (17, 1, 1), (17, 2, 2), (18, 1, 1),\n",
       "        (18, 2, 2), (19, 1, 1), (19, 2, 2), (20, 1, 1), (20, 2, 2),\n",
       "        (21, 1, 1), (21, 2, 2), (22, 1, 1), (22, 2, 2), (23, 1, 1),\n",
       "        (23, 2, 2), (24, 1, 1), (24, 2, 2), (25, 1, 1), (25, 2, 2),\n",
       "        (26, 1, 1), (26, 2, 2), (27, 1, 1), (27, 2, 2), (28, 1, 1),\n",
       "        (28, 2, 2), (29, 1, 1), (29, 2, 2), (30, 1, 1), (30, 2, 2),\n",
       "        (31, 1, 1), (31, 2, 2), (32, 1, 1), (32, 2, 2), (33, 1, 1),\n",
       "        (33, 2, 2), (34, 1, 1), (34, 2, 2), (35, 1, 1), (35, 2, 2),\n",
       "        (36, 1, 1), (36, 2, 2), (37, 1, 1), (37, 2, 2), (38, 1, 1),\n",
       "        (38, 2, 2), (39, 1, 1), (39, 2, 2), (40, 1, 1), (40, 2, 2),\n",
       "        (41, 1, 1), (41, 2, 2), (42, 1, 1), (42, 2, 2), (43, 1, 1),\n",
       "        (43, 2, 2), (44, 1, 1), (44, 2, 2), (45, 1, 1), (45, 2, 2),\n",
       "        (46, 1, 1), (46, 2, 2), (47, 1, 1), (47, 2, 2), (48, 1, 1),\n",
       "        (48, 2, 2), (49, 1, 1), (49, 2, 2), (50, 1, 1), (50, 2, 2)],\n",
       "       dtype=[('structure', '<i4'), ('center', '<i4'), ('neighbor', '<i4')])"
      ]
     },
     "execution_count": 671,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fock_bc.block(6).samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51400b6",
   "metadata": {},
   "source": [
    "### mean Wei Bin's tests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d397db",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### meanless hamiltonian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "623deb92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T14:29:49.047196Z",
     "start_time": "2022-12-28T14:29:49.035417Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tar = focks - np.mean(focks)\n",
    "tar_bc = couple_blocks(dense_to_blocks(tar, frames, orbs), cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "e7928132",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T14:29:49.400051Z",
     "start_time": "2022-12-28T14:29:49.395944Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0046]],\n",
       "\n",
       "        [[0.0046]],\n",
       "\n",
       "        [[0.0046]]])"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar_bc.block(7).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "c38a6220",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T14:29:49.937423Z",
     "start_time": "2022-12-28T14:29:49.924255Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4896)\n",
      "tensor(0.2537)\n"
     ]
    }
   ],
   "source": [
    "print(torch.linalg.norm( torch.tensor(_wddict1[2]) @ tar_bc.block(3).values[0] - tar_bc.block(3).values[1]))\n",
    "print(torch.linalg.norm( torch.tensor(_wddict2[2]) @ tar_bc.block(3).values[0] - tar_bc.block(3).values[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebddb22a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "this **DOESNT** work "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af46ecbe",
   "metadata": {},
   "source": [
    "#### meanless blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "cd8ee48e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T14:26:21.469407Z",
     "start_time": "2022-12-28T14:26:21.460891Z"
    }
   },
   "outputs": [],
   "source": [
    "ppblock = blocks.block(2)\n",
    "block_mean = torch.mean(ppblock.values, axis = 0)\n",
    "ppblock_m = (ppblock.values - block_mean).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "4b503520",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T14:26:55.985621Z",
     "start_time": "2022-12-28T14:26:55.977045Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pp_m_bc = cg.couple( ppblock_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "524aa2a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T14:28:39.108063Z",
     "start_time": "2022-12-28T14:28:39.105616Z"
    }
   },
   "outputs": [],
   "source": [
    "pp_m_bc = pp_m_bc[(1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "bbeb0077",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T14:28:39.766263Z",
     "start_time": "2022-12-28T14:28:39.760806Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2264)\n",
      "tensor(0.0990)\n"
     ]
    }
   ],
   "source": [
    "print(torch.linalg.norm( torch.tensor(_wddict1[2]) @ pp_m_bc[2][0] - pp_m_bc[2][1]))\n",
    "print(torch.linalg.norm( torch.tensor(_wddict2[2]) @ pp_m_bc[2][0] - pp_m_bc[2][2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe50798",
   "metadata": {},
   "source": [
    "#### stupid test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "ef45fe2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T14:32:36.511948Z",
     "start_time": "2022-12-28T14:32:36.499950Z"
    }
   },
   "outputs": [],
   "source": [
    "tar = []\n",
    "for i,f in enumerate(focks): \n",
    "    tar.append(f-f.mean())\n",
    "\n",
    "tar_bc = couple_blocks(dense_to_blocks(tar, frames, orbs), cg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "fc724135",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-28T14:32:58.453985Z",
     "start_time": "2022-12-28T14:32:58.449104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4536)\n",
      "tensor(0.2722)\n"
     ]
    }
   ],
   "source": [
    "print(torch.linalg.norm( torch.tensor(_wddict1[2]) @ tar_bc.block(3).values[0] - tar_bc.block(3).values[1]))\n",
    "print(torch.linalg.norm( torch.tensor(_wddict2[2]) @ tar_bc.block(3).values[0] - tar_bc.block(3).values[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f979dc60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fa907c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T11:34:36.376163Z",
     "start_time": "2022-12-27T11:34:36.368891Z"
    }
   },
   "source": [
    "## Jigyasa needs to modify samples for features to start from 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f93625",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-23T20:58:33.609633Z",
     "start_time": "2022-12-23T20:58:33.601609Z"
    }
   },
   "source": [
    "## Old and existing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b01d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T13:30:53.923834Z",
     "start_time": "2022-12-27T13:30:53.923814Z"
    }
   },
   "outputs": [],
   "source": [
    "def linear_init_weights(in_size, out_size):\n",
    "    m=torch.nn.Linear(in_size, out_size)\n",
    "    #m.weight.data.fill_(0)\n",
    "    m.bias.data.fill_(0)\n",
    "    return m \n",
    "\n",
    "class BlockModel(torch.nn.Module):\n",
    "    def __init__(self, frames, block, feat, optim, layer_size=None):\n",
    "        super().__init__()\n",
    "        self.frames = frames\n",
    "        self.nn = None\n",
    "        self.layer_size = layer_size\n",
    "        self.feature = feat\n",
    "        self.values = block\n",
    "        \n",
    "        \n",
    "    def initialize_model(self, seed):\n",
    "        \n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "        inputSize = self.feature.values.shape[-1] #feature dimension \n",
    "        outputSize = self.block.values.shape[0] #block sample size \n",
    "        \n",
    "        if self.layer_size is None: \n",
    "            self.nn = nn.Linear(inputSize, outputSize, bias = False)\n",
    "            \n",
    "        else:\n",
    "#             self.layer1 = nn.Linear(10,10)\n",
    "#             self.act1 = nn.TanH()\n",
    "#             self.layer2 = nn.Linear(10,100)\n",
    "#             self.act2 = nn.Sigmoid()\n",
    "            self.nn = torch.nn.Sequential(\n",
    "                linear_init_weights(inputSize, self.layer_size),\n",
    "                torch.nn.Tanh(),\n",
    "                linear_init_weights(self.layer_size, self.layer_size),\n",
    "                torch.nn.Tanh(),\n",
    "                linear_init_weights(self.layer_size, OutputSize),\n",
    "            )\n",
    "           \n",
    "        \n",
    "    def forward(self):\n",
    "        if self.nn is None:\n",
    "            raise Exception(\"call initialize_model first\")\n",
    "            \n",
    "        pred = self.nn(self.feature.values)\n",
    "#         pred = self.layer1(pred)\n",
    "#         pred = self.act1(pred)\n",
    "#         pred = self.layer2(pred)\n",
    "#         pred = self.act2(pred)\n",
    "#         pred = self.layer1(pred)\n",
    "        return pred \n",
    "\n",
    "    def fit(loss_function, optimizer_type, lr, reg, nepochs = 5000):\n",
    "        if optimizer_type == \"Adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr = lr, weight_decay = self.reg.item())\n",
    "            threshold = 20000\n",
    "            scheduler_threshold = 20000\n",
    "            tol = 1e-3\n",
    "            \n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, scheduler_threshold, gamma = 0.5)\n",
    "        best_state = copy.deepcopy(self.nn.state_dict())\n",
    "        \n",
    "        for epoch in tqdm(nepochs):\n",
    "            optimizer.zero_grad()\n",
    "            pred = self.nn(self.feature.values)\n",
    "            loss = loss_function(self.values, pred, self.frames, )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd20257f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-12-27T13:30:53.930133Z",
     "start_time": "2022-12-27T13:30:53.930118Z"
    }
   },
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, coupled_blocks, features, weights=None, intercepts=None):\n",
    "        super().__init__()\n",
    "        self.coupled_blocks = coupled_blocks\n",
    "        self.features = features\n",
    "        self.weights = {}\n",
    "        if weights==None:\n",
    "            for idx_fock, block_fock in self.coupled_blocks:\n",
    "                block_type, ai, ni, li, aj, nj, lj, L = idx_fock\n",
    "                parity= (-1)**(li+lj+L)\n",
    "                size = self.features.block(block_type=block_type, spherical_harmonics_l=L,inversion_sigma=parity, \n",
    "                                       species_center=ai, species_neighbor=aj).values.shape[2]\n",
    "                #self.weights[idx_fock] = torch.nn.Parameter(torch.zeros(size, dtype=torch.float64))\n",
    "                self.weights[idx_fock] = torch.nn.Parameter(torch.randn(size, dtype=torch.float64))\n",
    "            \n",
    "        else: \n",
    "            self.weights = weights\n",
    "        \n",
    "        self.intercepts = {}\n",
    "        if intercepts is None:\n",
    "            for idx_fock, block_fock in self.coupled_blocks:\n",
    "                block_type, ai, ni, li, aj, nj, lj, L = idx_fock\n",
    "                parity= (-1)**(li+lj+L)\n",
    "                if L==0 and parity==1 and block_type==0:\n",
    "                    self.intercepts[idx_fock] = torch.nn.Parameter(torch.randn(1, dtype=torch.float64))\n",
    "                else:\n",
    "                    self.intercepts[idx_fock] = 0\n",
    "        else:\n",
    "            self.intercepts = intercepts\n",
    "         \n",
    "    def forward(self, features):\n",
    "        k = []\n",
    "        pred_blocks = []\n",
    "        for (idx, wts) in self.weights.items():\n",
    "            #print(wts)\n",
    "            block_type, ai, ni, li, aj, nj, lj, L = idx\n",
    "            k.append(list(idx))\n",
    "            parity= (-1)**(li+lj+L)\n",
    "            X = features.block(block_type=block_type, spherical_harmonics_l=L,inversion_sigma=parity, \n",
    "                                   species_center=ai, species_neighbor=aj)\n",
    "            X_new = torch.from_numpy(X.values.reshape(-1, X.values.shape[2]))\n",
    "            #print(idx, wts.shape, X.values.shape, X_new.shape)\n",
    "            Y = X_new @ wts + self.intercepts[idx]\n",
    "            \n",
    "            newblock = TensorBlock(\n",
    "                        values=Y.reshape((-1, 2 * L + 1, 1)),\n",
    "                        samples=X.samples,\n",
    "                        components=[Labels(\n",
    "                            [\"mu\"], np.asarray(range(-L, L + 1), dtype=np.int32).reshape(-1, 1)\n",
    "                        )],\n",
    "                        properties= Labels([\"values\"], np.asarray([[0]], dtype=np.int32))\n",
    "                    )\n",
    "            pred_blocks.append(newblock) \n",
    "        \n",
    "        keys = Labels(('block_type', 'a_i', 'n_i', 'l_i', 'a_j', 'n_j', 'l_j', 'L'), np.asarray(k, dtype=np.int32))\n",
    "        pred_fock = TensorMap(keys, pred_blocks)\n",
    "        return(pred_fock)\n",
    "        ### add direct eigenvalue prediction here as well\n",
    "    \n",
    "    def parameters(self):\n",
    "        for idx, wts in self.weights.items():\n",
    "            yield wts\n",
    "        for idx, wts in self.intercepts.items():\n",
    "            if type(wts) is not int:\n",
    "                yield wts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45786c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "786px",
    "left": "27px",
    "top": "111.133px",
    "width": "213px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
